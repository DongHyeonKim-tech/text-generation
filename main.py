from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import json
import re

app = FastAPI(title="Text Generation API", description="AI í…ìŠ¤íŠ¸ ìƒì„± API")

# model_id = "openai/gpt-oss-20b"

# # MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° device ì„¤ì •
# device = "mps" if torch.backends.mps.is_available() else "cpu"
# print(f"Using device: {device}")

# # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì „ì—­ìœ¼ë¡œ ë¡œë“œ
# tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_id,
#     torch_dtype="auto",
#     device_map="auto"
# )

class TextRequest(BaseModel):
    prompt: str

class TextResponse(BaseModel):
    generated_text: str

# def generate_text(prompt: str):
#     try:
#         inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to(model.device)

#         # ì…ë ¥ ê¸¸ì´ì™€ ì»¨í…ìŠ¤íŠ¸ ì—¬ìœ  ê³„ì‚° (ê¶Œì¥: ì—¬ìœ  32~64 í† í°)
#         max_ctx = getattr(model.config, "max_position_embeddings", 8192)  # ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„
#         inp_len = inputs.input_ids.shape[1]
#         safe_room = 64
#         max_new = min(512, max_ctx - inp_len - safe_room)  # ì—¬ìœ  ë‚´ì—ì„œ ìƒì„±ê¸¸ì´ ì„¤ì •
#         max_new = max(64, max_new)  # ìµœì†Œ 64 ë³´ì¥

#         gen_out = model.generate(
#             **inputs,
#             max_new_tokens=max_new,        # ì¶©ë¶„íˆ í¬ê²Œ
#             min_new_tokens=32,             # ë„ˆë¬´ ì§§ê²Œ ì•ˆ ëŠê¸°ê²Œ
#             do_sample=True,                # í•„ìš” ì‹œ Falseë¡œ ë°”ê¿”ë„ ë¨(ê²°ì •ë¡ ì )
#             temperature=0.7,
#             top_p=0.9,
#             eos_token_id=tokenizer.eos_token_id,
#             pad_token_id=tokenizer.eos_token_id,
#             no_repeat_ngram_size=3,        # ë°˜ë³µ ì¤„ì´ê¸°(ì„ íƒ)
#             repetition_penalty=1.05,       # ë°˜ë³µ ì¤„ì´ê¸°(ì„ íƒ)
#             return_dict_in_generate=True,
#             output_scores=True,
#         )

#         # í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•œ "ì™„ì„± ë¶€ë¶„"ë§Œ ë””ì½”ë“œ
#         generated_ids = gen_out.sequences[0]
#         completion_ids = generated_ids[inp_len:]  # ì—¬ê¸°ì„œë¶€í„°ê°€ ì‹¤ì œ answer
#         answer = tokenizer.decode(completion_ids, skip_special_tokens=True).strip()
        
#         return answer
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

MODEL_ID = "openai/gpt-oss-20b"

tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype="auto",
    device_map="auto",
)

# ğŸ” ë§¤ í˜¸ì¶œë§ˆë‹¤ ë§Œë“¤ì§€ ë§ê³  ì „ì—­ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±
gen = pipeline(
    "text-generation",
    model=model,
    tokenizer=tok,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LENGTH_PRESETS = {
    "short":  {"max_new": 128, "min_new": 24,  "max_chars": 350},
    "medium": {"max_new": 384, "min_new": 64,  "min_chars": 200},
    "long":   {"max_new": 768, "min_new": 128, "min_chars": 450},
}

SHORT_HINTS  = ["í•œì¤„", "ì§§ê²Œ", "ê°„ë‹¨íˆ", "ìš”ì•½", "tl;dr"]
LONG_HINTS   = ["ìì„¸íˆ", "ê¸¸ê²Œ", "ì˜ˆì‹œ", "ì½”ë“œ", "ë‹¨ê³„ë³„", "íŠœí† ë¦¬ì–¼", "ìƒì„¸íˆ"]

def _detect_length(prompt: str) -> str:
    p = prompt.lower()
    if any(h in prompt for h in SHORT_HINTS): return "short"
    if any(h in prompt for h in LONG_HINTS):  return "long"
    return "medium"

def _korean_ratio(text: str) -> float:
    # í•œê¸€ ë¬¸ì ë¹„ìœ¨ë¡œ ëŒ€ì¶© ê°ì§€
    hangul = re.findall(r"[ê°€-í£]", text)
    return len(hangul) / max(1, len(text))

def _shorten_to_sentences(text: str, n: int = 2) -> str:
    # '.' '!' '?' ê¸°ì¤€ìœ¼ë¡œ ì• në¬¸ì¥ë§Œ ë°˜í™˜ (í•œêµ­ì–´ ë§ˆì¹¨í‘œ í¬í•¨)
    parts = re.split(r"(?<=[\.!?ã€‚ï¼?])\s+", text.strip())
    return " ".join(parts[:n]).strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_text_from_pipeline(prompt: str, length: str = "auto", force_korean: bool = True, verbose: bool = False) -> str:
    """
    length: "auto" | "short" | "medium" | "long"
    - auto: í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œë¡œ ê¸¸ì´ ì¶”ì •
    - force_korean: í•œêµ­ì–´ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ 1íšŒ ì¬ìƒì„±í•˜ì—¬ í•œêµ­ì–´ë¡œë§Œ ë‹¤ì‹œ ìš”ì²­
    """
    if length == "auto":
        length = _detect_length(prompt)
    preset = LENGTH_PRESETS[length]

    sys = "ë‹¹ì‹ ì€ í•­ìƒ í•œêµ­ì–´ë¡œë§Œ, ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•©ë‹ˆë‹¤."
    # í•œêµ­ì–´ ê³ ì • ê°•í™” ë¬¸ì¥
    lang_rule = "ë°˜ë“œì‹œ 100% í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´ ìš©ì–´ê°€ í•„ìš”í•˜ë©´ ê´„í˜¸ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ í’€ì´í•˜ì„¸ìš”."

    base_prompt = f"{sys}\n{lang_rule}\n\nì§ˆë¬¸: {prompt}\në‹µë³€:"
    if verbose:
        print("[pipeline_prompt]", base_prompt)

    # 1ì°¨ ìƒì„±
    out = gen(
        base_prompt,
        max_new_tokens=preset["max_new"],
        min_new_tokens=preset["min_new"],
        temperature=0.7,
        top_p=0.95,
        do_sample=True,
        no_repeat_ngram_size=3,
        repetition_penalty=1.05,
        return_full_text=False,  # â† í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ê²°ê³¼ë§Œ
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.eos_token_id,
    )
    text = out[0]["generated_text"].strip()

    # ê¸¸ì´ ë³´ì •: shortëŠ” ë„ˆë¬´ ê¸¸ë©´ 2ë¬¸ì¥ìœ¼ë¡œ ì»·, medium/longì€ ë„ˆë¬´ ì§§ìœ¼ë©´ 1íšŒ ì¬ì‹œë„
    if length == "short":
        if len(text) > preset["max_chars"]:
            text = _shorten_to_sentences(text, n=2)

    else:
        min_chars = preset.get("min_chars", 0)
        if len(text) < min_chars:
            # ë” ìì„¸íˆ ìš”ì²­í•˜ì—¬ 1íšŒ ì¬ìƒì„±
            regen_prompt = (
                f"{sys}\n{lang_rule}\n\n"
                f"ì§ˆë¬¸: {prompt}\n"
                f"ì§€ì‹œì‚¬í•­: ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë” ìì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ, ì˜ˆì‹œ/í•­ëª©/ë‹¨ê³„ ë“±ì„ í¬í•¨í•´ ì„¤ëª…í•˜ì„¸ìš”. ë°˜ë³µì€ í”¼í•˜ê³  ìƒˆë¡œìš´ ì •ë³´ì™€ êµ¬ì¡°í™”ë¥¼ ì œê³µí•˜ì„¸ìš”.\n"
                f"ë‹µë³€:"
            )
            if verbose:
                print("[regen_prompt: too short]")

            out2 = gen(
                regen_prompt,
                max_new_tokens=min(1024, preset["max_new"] * 2),
                min_new_tokens=max(64, preset["min_new"]),
                temperature=0.7,
                top_p=0.95,
                do_sample=True,
                no_repeat_ngram_size=3,
                repetition_penalty=1.05,
                return_full_text=False,
                eos_token_id=tok.eos_token_id,
                pad_token_id=tok.eos_token_id,
            )
            text = out2[0]["generated_text"].strip()

    # í•œêµ­ì–´ ê°•ì œ: í•œê¸€ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ 1íšŒ ì¬ì‘ì„±
    if force_korean and _korean_ratio(text) < 0.5:
        fix_prompt = (
            f"{sys}\n{lang_rule}\n\n"
            f"ë‹¤ìŒ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œë§Œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì˜ì–´ í‘œê¸°ëŠ” ì œê±°í•˜ê³  ë§¥ë½ì„ ìœ ì§€í•˜ì„¸ìš”.\n\n"
            f"ë‚´ìš©:\n{text}\n\n"
            f"í•œêµ­ì–´ ë‹µë³€:"
        )
        if verbose:
            print("[regen_prompt: force korean]")

        out3 = gen(
            fix_prompt,
            max_new_tokens=min(1024, preset["max_new"] * 2),
            min_new_tokens=64,
            temperature=0.7,
            top_p=0.95,
            do_sample=True,
            no_repeat_ngram_size=3,
            repetition_penalty=1.05,
            return_full_text=False,
            eos_token_id=tok.eos_token_id,
            pad_token_id=tok.eos_token_id,
        )
        text = out3[0]["generated_text"].strip()

    return text
# @app.post("/generate", response_model=TextResponse)
# async def generate_text_endpoint(request: TextRequest):
#     generated_text = generate_text(request.prompt)
#     return TextResponse(generated_text=generated_text)

@app.post("/generate_pipeline", response_model=TextResponse)
async def generate_text_endpoint(request: TextRequest):
    generated_text = get_text_from_pipeline(request.prompt)
    return TextResponse(generated_text=generated_text)

@app.get("/")
async def root():
    return {"message": "Text Generation APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8012)
